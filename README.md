## LLM-Generated Code vs Human Code Benchmarking

### Overview
This project evaluates the performance and correctness of LLM-generated code compared to human-written implementations.

### Objectives
- Measure runtime performance
- Analyze memory usage
- Validate correctness on algorithmic problems

### Methodology
Implemented automated benchmarking and testing pipelines using Python.

### Technologies
- Python
- Large Language Models
- Algorithms
- Performance Analysis

### Project Type
Academic / Research Project
